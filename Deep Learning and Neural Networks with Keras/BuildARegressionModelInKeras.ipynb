{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM: Introduction to Deep Learning & Neural Networks Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Regression Model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Build a baseline model.\n",
    "Use the Keras library to build a neural network with the following:\n",
    "- One hidden layer of 10 nodes, and a ReLU activation function\n",
    "\n",
    "- Use the adam optimizer and the mean squared error  as the loss function.\n",
    "\n",
    "1. Randomly split the data into a training and test sets by holding 30% of the data for testing.\n",
    "\n",
    "2. Train the model on the training data using 50 epochs.\n",
    "\n",
    "3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n",
    "\n",
    "4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n",
    "\n",
    "5. Report the mean and the standard deviation of the mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "concrete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to predictors and target\n",
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of predictors as inputs for network\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,))) # first layer must match number of inputs\n",
    "    model.add(Dense(1)) # One output value\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model = regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 114.4249\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 103.4356\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 108.4610\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 107.3452\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 97.4045\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 99.8988\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 116.3092\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 95.9741\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 96.5056\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 83.5513\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 77.8950\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 75.4074\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 67.4538\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 72.1227\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 63.5522\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 59.8561\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 60.6989\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 55.2161\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 52.3174\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 55.8841\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 50.4269\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 51.0927\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 45.0105\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 50.1745\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 43.1124\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 53.1243\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 52.3707\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 52.2322\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 48.1942\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 49.2990\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 47.2241\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 56.5998\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 59.4916\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 60.9519\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 49.5954\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 50.1980\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 52.4722\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 52.1331\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 52.7498\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 47.3710\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 56.9209\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 51.2438\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 53.9712\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 49.6203\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 51.9726\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 57.1246\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 49.3398\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 54.6422\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 56.4406\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 53.9751\n",
      "Mean MSE: 64.45578796386718\n",
      "Standard Deviation: 20.524928457567043\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i in range(50):\n",
    "    # Step 1: Randomly split dataset to training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3)\n",
    "\n",
    "    # Step 2: Train the model on the training data using 50 epochs\n",
    "    model.fit(X_train, y_train, verbose=0, epochs=50)\n",
    "\n",
    "    # Step 3: Evaluate the model on the test data and compute the mean squared error\n",
    "    errors.append(model.evaluate(X_test, y_test))\n",
    "print('Mean MSE: {}\\nStandard Deviation: {}'.format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Normalize the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat Part A but use a normalized version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize predictors\n",
    "\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 208.8373\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 134.1962\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 120.7043\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 102.3880\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 96.4200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 85.6521\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 76.0380\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 75.9684\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 65.0370\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 60.3617\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 57.5488\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 56.7008\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 55.8203\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 52.2984\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 50.0142\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 49.3645\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 44.0813\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 44.4643\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 41.6727\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 43.4638\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 46.3848\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.9455\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 43.5432\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 41.2758\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 43.2550\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.1743\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 43.9635\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.3480\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 41.6038\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.0963\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 42.8341\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 40.0298\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.7593\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.6189\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 42.9119\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 37.2160\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 46.1039\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 40.5904\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.2056\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 38.1971\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.8810\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 38.4421\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 32.2871\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 34.1440\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.6788\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 32.2243\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.7016\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.5642\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.2973\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.3602\n",
      "Mean MSE: 53.87340133666992\n",
      "Standard Deviation: 31.211576387766904\n"
     ]
    }
   ],
   "source": [
    "# Re-run the model on normalized predictors\n",
    "\n",
    "errors = []\n",
    "for i in range(50):\n",
    "    # Step 1: Randomly split dataset to training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "    # Step 2: Train the model on the training data using 50 epochs\n",
    "    model.fit(X_train, y_train, verbose=0, epochs=50)\n",
    "\n",
    "    # Step 3: Evaluate the model on the test data and compute the mean squared error\n",
    "    errors.append(model.evaluate(X_test, y_test))\n",
    "print('Mean MSE: {}\\nStandard Deviation: {}'.format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data decreased the MSE by about 16% from Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Increase the number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat Part B but use 100 epochs this time for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 42.7213\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 36.5901\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 45.0921\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 37.5337\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 37.0568\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 37.2752\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 36.9427\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.3483\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 37.4408\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.0976\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.8060\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.1526\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.9920\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 37.3146\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 39.8833\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 38.8929\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 39.5542\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.0895\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 38.9509\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 36.6949\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 34.7480\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 34.4565\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 34.8677\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 33.8333\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 43.2281\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 39.7338\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 37.6697\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.9545\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 38.4003\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 38.9396\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.9133\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.6892\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.9187\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 41.1314\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 38.6309\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.2413\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.7659\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 41.7628\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 35.8922\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 35.1155\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 37.7981\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 33.5721\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 39.2461\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 34.4980\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.6595\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 38.4631\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.5067\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 35.5785\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 33.8886\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.8144\n",
      "Mean MSE: 37.40694549560547\n",
      "Standard Deviation: 2.475810510526528\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i in range(50):\n",
    "    # Step 1: Randomly split dataset to training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "    # Step 2: Train the model on the training data using 50 epochs\n",
    "    model.fit(X_train, y_train, verbose=0, epochs=100)\n",
    "\n",
    "    # Step 3: Evaluate the model on the test data and compute the mean squared error\n",
    "    errors.append(model.evaluate(X_test, y_test))\n",
    "print('Mean MSE: {}\\nStandard Deviation: {}'.format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the epochs from 50 to 100 reduced the MSE by about 30% from Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Increase the number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat part B but use a neural network with three hidden layers, each with 10 nodes and a ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_model():\n",
    "        # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,))) # first layer must match number of inputs\n",
    "    model.add(Dense(10, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"relu\"))\n",
    "    model.add(Dense(1)) # One output value\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  0\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 150.9135\n",
      "Run  1\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 101.4764\n",
      "Run  2\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 48.7399\n",
      "Run  3\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 48.8796\n",
      "Run  4\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 43.1675\n",
      "Run  5\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 39.0043\n",
      "Run  6\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 32.9719\n",
      "Run  7\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 36.8604\n",
      "Run  8\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 38.5849\n",
      "Run  9\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 35.4687\n",
      "Run  10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 31.0608\n",
      "Run  11\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 32.7714\n",
      "Run  12\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.0589\n",
      "Run  13\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 25.9900\n",
      "Run  14\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 30.6579\n",
      "Run  15\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.5967\n",
      "Run  16\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 32.3466\n",
      "Run  17\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 26.3400\n",
      "Run  18\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.9817\n",
      "Run  19\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 26.9763\n",
      "Run  20\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 28.6710\n",
      "Run  21\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 31.0923\n",
      "Run  22\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 26.8730\n",
      "Run  23\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 26.4371\n",
      "Run  24\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 24.3489\n",
      "Run  25\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 31.4333\n",
      "Run  26\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 32.3381\n",
      "Run  27\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 31.4221\n",
      "Run  28\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 29.2975\n",
      "Run  29\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.2322\n",
      "Run  30\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.5157\n",
      "Run  31\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.6173\n",
      "Run  32\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 25.1554\n",
      "Run  33\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 25.5600\n",
      "Run  34\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 26.1063\n",
      "Run  35\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 24.9311\n",
      "Run  36\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 22.2168\n",
      "Run  37\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 25.2022\n",
      "Run  38\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 22.1144\n",
      "Run  39\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 21.4746\n",
      "Run  40\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 24.2754\n",
      "Run  41\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 18.4794\n",
      "Run  42\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 19.2465\n",
      "Run  43\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 22.4786\n",
      "Run  44\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 27.2212\n",
      "Run  45\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 20.7033\n",
      "Run  46\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 24.0478\n",
      "Run  47\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 22.4676\n",
      "Run  48\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 19.7203\n",
      "Run  49\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 21.3464\n",
      "Mean MSE: 32.43746513366699\n",
      "Standard Deviation: 20.841952452986124\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation of the new model\n",
    "model = three_layer_model()\n",
    "errors = []\n",
    "for i in range(50):\n",
    "    print('Run ', i)\n",
    "    # Step 1: Randomly split dataset to training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "    # Step 2: Train the model on the training data using 50 epochs\n",
    "    model.fit(X_train, y_train, verbose=0, epochs=50)\n",
    "\n",
    "    # Step 3: Evaluate the model on the test data and compute the mean squared error\n",
    "    errors.append(model.evaluate(X_test, y_test))\n",
    "print('Mean MSE: {}\\nStandard Deviation: {}'.format(np.mean(errors), np.std(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two more hidden layers reduced the error by about 40% from part B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
